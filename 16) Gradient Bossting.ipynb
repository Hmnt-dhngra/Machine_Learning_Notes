{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "380d79c8",
   "metadata": {},
   "source": [
    "#### **Gradient Boosting**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8914c9c9",
   "metadata": {},
   "source": [
    "**What is Gradient Boosting?**\n",
    "\n",
    "Gradient Boosting is an ensemble learning technique that builds a strong predictive model by sequentially combining multiple weak learners, typically shallow decision trees (often called decision stumps or small CART trees).\n",
    "\n",
    "Unlike bagging-based methods (e.g., Random Forest), where models are trained independently and in parallel, gradient boosting trains models sequentially, with each new model focusing on correcting the mistakes made by the previous ensemble.\n",
    "\n",
    "\n",
    "**Working**\n",
    "- Start with an initial model (often a simple constant prediction).\n",
    "- Compute the errors (residuals) of the current model.\n",
    "- Train a new weak learner to predict these residuals.\n",
    "- Add this learner to the existing model with a scaling factor (learning rate).\n",
    "- Repeat the process for a fixed number of iterations or until convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bbf2af7",
   "metadata": {},
   "source": [
    "**Why is it needed?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82607621",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
