{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c95223a",
   "metadata": {},
   "source": [
    "Whenever we train a machine learning model we divide the dataset (1000 rows) into 2 parts (say 70-30 split):\n",
    "- Training : Used for training the model and for hyperparameter tuning (by dividing the training data further into train and validation)\n",
    "- Test : To check the performance of the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4825d18",
   "metadata": {},
   "source": [
    "**Cross Validation**\n",
    "\n",
    "Cross-validation is a **model evaluation technique** used in machine learning to estimate how well a model will generalize to unseen data. \n",
    "\n",
    "Instead of evaluating the model on a single trainâ€“test split, cross-validation repeatedly trains and tests the model on different subsets of the data and then aggregates the results.\n",
    "\n",
    "At a high level:\n",
    "\n",
    "- The dataset is split into multiple parts (called folds).\n",
    "- The model is trained on some folds and validated on the remaining fold.\n",
    "- This process is repeated so that each fold is used once as the validation set.\n",
    "- The performance metrics are averaged across all runs.\n",
    "\n",
    "\n",
    "The most common type is **K-fold cross validation** which splits the data into k parts, each used once for testing. \n",
    "\n",
    "The other type is **stratified k-fold** which ensures class balanced in each fold. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f9c5f84",
   "metadata": {},
   "source": [
    "**Steps**\n",
    "- Data\n",
    "- EDA \n",
    "- FE \n",
    "- cross validation split (gives general model accuracy) -> train accuracy & test accuracy. If need be you can do feature engineering further to enhance the general accuracy. \n",
    "- Model creation on Train + test data (using ROC AUC curve defining threshold value) for deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b8f3ae",
   "metadata": {},
   "source": [
    "Types of Cross validation : \n",
    "1) Leave one out CV (LOOCV) : \n",
    "- Say you have 500 records initially identified as training data. \n",
    "- With LOOCV, we take all the 500 records\n",
    "- For experiment 1: we take 499 as the final train and leave 1 for the validation set. Here you get say accuracy A1. \n",
    "- For experiment 2: we take **SOME OTHER** 499 as the final train and leave **some other** 1 for the validation set. Here you get say accuracy A2. \n",
    "**Disadvantage** : Complexity increases with the size of data. Model leads to overfitting. \n",
    "\n",
    "\n",
    "2) Leave p out CV : same as above, it's just you leave p records out for validation contrary to just 1 mentioned above. \n",
    "\n",
    "3) K-fold cross validation : Say you have a total of n=500 records and for k=5 , you will have 500/5=100 records. \n",
    "    - For experiment 1: we will have first 100 records as the validation data, rest 400 as the test data\n",
    "    - For experiment 2: we will have second 100 records as the validation data, rest 400 as the test data\n",
    "\n",
    "4) Stratified K -fold cross validation : There may be scenarios wherein only specific categories are in majority in the training or test data. This method ensures to have thevalidation data as somewhat having a balanced dataset. \n",
    "\n",
    "5) Time series cross validation : Cross validation is done on order of dates. This is usually done when we have a time series and we changed something like added features based on poor reviews and then possibly have better reviews over a range of time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914587fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
