{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0759e155",
   "metadata": {},
   "source": [
    "**Support Vector Machines** \n",
    "- It is a supervsied machine learning algorithm used for both classification and regression. \n",
    "- This is however mainly used for classficiation problems.  \n",
    "\n",
    "Now Logistic regression could have done that but Logistic regression does not guarantee maximum separation\n",
    "\n",
    "- Many hyperplanes can separate data\n",
    "- LR picks one based on likelihood, not margin\n",
    "- SVM explicitly finds the best-separated boundary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dbd3efe",
   "metadata": {},
   "source": [
    "**Why did you need SVM when you had Logistic Regression**\n",
    "\n",
    "- Fundamentally speaking Logistic regression models probabilities, whereas SVM focusses on finding the best separating boundary. \n",
    "\n",
    "- Logistic Regression by default has a linear decision boundary. It needs manual feature engineering for non-linearity. SVM uses kernel trick (RBF, polynomial, sigmoids) and can create non linear boundaries in original space without explicit feature construction. \n",
    "\n",
    "\n",
    "- Logistic Regression tries to fit every point hence is highly sensitive to outliers, while SVM only cares about the critical boundary points, hence with SVM sensitivity to outliers are controlled via margins. \n",
    "\n",
    "- SVM has a better performance for high dimensional data. \n",
    "------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5e454a",
   "metadata": {},
   "source": [
    "SVM creates margins that separate two types of data. \n",
    "\n",
    "Whenever a new data point comes within the margins, it is usually predicted with low confidence but anything beyind the margins is predicted with high confidence \n",
    "\n",
    "Margin usually intersects one of the data points on either sides which rae called support vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f323ab",
   "metadata": {},
   "source": [
    "- WX+b > 0 : the point lies above the line\n",
    "- WX+b < 0 : the point lies below the line\n",
    "- WX+b = 0 : the point lies on the line\n",
    "\n",
    "    - score received is directly proportional to the distance of the point from the boundary. a score of +0.1 states it is on the positive side of the line but close\n",
    "    -  score received is directly proportional to the distance of the point from the boundary. a score of +100 states it is on the positive side of the line but far away"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a15dbbe",
   "metadata": {},
   "source": [
    "SVM doesn't just want a line where score > 0 and lets you categorise blue or green. It actually wants to find the line such that every point is atleast some minimum distance away from the boundary. This is why we have a concept of **margins**\n",
    "\n",
    "- Positive point must have a score of >= +1 \n",
    "- Negative point must have a score of <= -1\n",
    "\n",
    "The points which are exactly at +1 or -1 are the points that have the lowest acceptable confidence. These are support vectors. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba4f7425",
   "metadata": {},
   "source": [
    "**The goal is to minimise the loss which is dependant on | w |, ultimately we will minimise 1/2 * | w |^2**\n",
    "\n",
    "**y(wx+b)>=1**\n",
    "\n",
    "For hard margin for perfect separation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b245929",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "**What is SVM (Hard Margin Case)?**\n",
    "\n",
    "A Support Vector Machine is a supervised learning algorithm that finds a decision boundary (hyperplane) that separates two classes while maximizing the margin, i.e., the distance between the closest data points of each class and the hyperplane.\n",
    "\n",
    "In the **hard-margin** setting, SVM imposes **zero tolerance for misclassification.**\n",
    "\n",
    "**Limitation of Hard Margin SVM**\n",
    "\n",
    "Hard margin SVM assumes:\n",
    "\n",
    "- Data is perfectly linearly separable\n",
    "- No misclassification is allowed\n",
    "- All points must lie outside the margin\n",
    "\n",
    "**Why this fails in practice**\n",
    "\n",
    "- Real data is noisy because of measurement errors, Label noise, etc. \n",
    "- Outliers exist: A single extreme point can force a very poor decision boundary\n",
    "- Non-separable data: Many datasets simply cannot be separated by a hyperplane\n",
    "\n",
    "**Result** \n",
    "Hard margin SVM either fails to find a solution or produces a boundary that **overfits** badly.\n",
    "\n",
    "---------------------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce084d9",
   "metadata": {},
   "source": [
    "**Need of a soft margin**\n",
    "\n",
    "A soft margin was introduced in SVM to make the algorithm usable and robust on real-world data, where the assumptions of a hard margin SVM almost never hold.\n",
    "\n",
    "**What Soft Margin SVM Changes**\n",
    "\n",
    "Soft margin SVM relaxes the strict separation constraint by allowing controlled violations.\n",
    "This allows:\n",
    "- points inside the margin \n",
    "- points on the wrong side of the hyperplane (misclassified)\n",
    "---------------------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa49243e",
   "metadata": {},
   "source": [
    "There are two components of the loss functioin here \n",
    "\n",
    "- Maximises margin and makes it as wide as possible. \n",
    "- Hinge loss is responsible for penalising points that violates the margin. This has a component called \"C\". This C contols the trade-off.\n",
    "    - A large value of C : Heavily penalises error -> behaves like hard margin\n",
    "    - Small C : Allows more violations -> wider margin, better generalisation and allows misclaffication. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f66b5442",
   "metadata": {},
   "source": [
    "Margin is not a learnable parameter, it's a concept that helps us mesaure how far the support vectors(closest points) are from the separating line'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3649aa1",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "**SVM Kernels**\n",
    "\n",
    "Kernels are a mathematical mechanism that allow the algorithm to model non-linear decision boundaries while still operating as if it were a linear classifier.\n",
    "\n",
    "**Why kernels are needed?**\n",
    "\n",
    "A standard (hard or soft margin) SVM constructs a **linear separating hyperplane**:\n",
    "wâ‹…x+b=0\n",
    "\n",
    "This works well **only when the data is linearly separable** (or nearly so).\n",
    "However, many **real-world datasets are not linearly separable** in the original feature space.\n",
    "\n",
    "The kernel idea solves this problem by:\n",
    "\n",
    "- Mapping input data from the original space to a higher-dimensional feature space\n",
    "- Making the data linearly separable in that transformed space\n",
    "\n",
    "SVM kernel is a function that measures similarity between two data points in an implicit feature space. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e200c3d6",
   "metadata": {},
   "source": [
    "Here we use kernel functions to convert non linear say 2D data by passing through a kernel function and yield somethingh which is better for computation usually of higher dimesnions. Kernel function works like dot product and hence finds similarity scores. Higher the score, nearer the features\n",
    "\n",
    "Kernel types: \n",
    "- Linear kernels :  No feature transformation, equivalent to linear SVM, Fast and scalable. Useful when data is approximately linearly separable or is a high dimesnaional data \n",
    "- Polynomial : \n",
    "- Radial basic function (RBF) : \n",
    "- Sigmoid : \n",
    "\n",
    "\n",
    "Kernel functions doesn't save the new dimensions but uses it just for calculation. Ultimately it saves only the similarity scores. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a76ebc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f99e33fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = load_iris()\n",
    "X = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "y = pd.Series(iris.target, name='species')\n",
    "# setosa\n",
    "# versicolor\n",
    "# virginica\n",
    "\n",
    "# sepal length (cm)\n",
    "# sepal width (cm)\n",
    "# petal length (cm)\n",
    "# petal width (cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7be3f26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (150, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)\n",
       "0                5.1               3.5                1.4               0.2\n",
       "1                4.9               3.0                1.4               0.2\n",
       "2                4.7               3.2                1.3               0.2\n",
       "3                4.6               3.1                1.5               0.2\n",
       "4                5.0               3.6                1.4               0.2"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_df = X.copy()\n",
    "X_df['Actual'] = y\n",
    "\n",
    "print(\"Dataset shape:\", X.shape)\n",
    "\n",
    "X.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "42d5452e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "      <th>Actual</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n",
       "0                5.1               3.5                1.4               0.2   \n",
       "1                4.9               3.0                1.4               0.2   \n",
       "2                4.7               3.2                1.3               0.2   \n",
       "3                4.6               3.1                1.5               0.2   \n",
       "4                5.0               3.6                1.4               0.2   \n",
       "\n",
       "   Actual  \n",
       "0       0  \n",
       "1       0  \n",
       "2       0  \n",
       "3       0  \n",
       "4       0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fff2a982",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target classes: ['setosa' 'versicolor' 'virginica']\n",
      "{0: 'setosa', 1: 'versicolor', 2: 'virginica'}\n"
     ]
    }
   ],
   "source": [
    "print(\"Target classes:\", iris.target_names)\n",
    "\n",
    "target_mapping = {i: str(name) for i, name in enumerate(iris.target_names)}\n",
    "print(target_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9411e709",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c297f67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "148d0bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# linear_svm = SVC(kernel=\"linear\", C=1.0, random_state=20)\n",
    "\n",
    "svm_models = {\n",
    "    \"Linear\": SVC(kernel=\"linear\", C=1.0, random_state=20),\n",
    "    \"Polynomial\": SVC(kernel=\"poly\", degree=3, C=1.0, random_state=20),\n",
    "    \"RBF\": SVC(kernel=\"rbf\", C=1.0, gamma=\"scale\", random_state=20)\n",
    "}\n",
    "\n",
    "results = [] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9117570d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear SVM:\n",
      "  Train Accuracy: 0.9750\n",
      "  Test Accuracy:  1.0000\n",
      "==============================\n",
      "Polynomial SVM:\n",
      "  Train Accuracy: 0.9333\n",
      "  Test Accuracy:  0.9000\n",
      "==============================\n",
      "RBF SVM:\n",
      "  Train Accuracy: 0.9750\n",
      "  Test Accuracy:  0.9667\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "for name, model in svm_models.items():\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    y_train_pred = model.predict(X_train_scaled)\n",
    "    y_test_pred = model.predict(X_test_scaled)\n",
    "    train_acc = accuracy_score(y_train, y_train_pred)\n",
    "    test_acc = accuracy_score(y_test, y_test_pred)\n",
    "    results.append([name, train_acc, test_acc])\n",
    "    print(f\"{name} SVM:\")\n",
    "    print(f\"  Train Accuracy: {train_acc:.4f}\")\n",
    "    print(f\"  Test Accuracy:  {test_acc:.4f}\")\n",
    "    print(\"===\"*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a43f55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b2e9f1e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
