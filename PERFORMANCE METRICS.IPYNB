{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c497c7b",
   "metadata": {},
   "source": [
    "**Types of evaluation metrics (FOR REGRESSION):** \n",
    "\n",
    "**Aim** : Judge the accuracy of the model's predictions versus the actual value. \n",
    "\n",
    "*Error = Actual Value - Predicted value*\n",
    "\n",
    "- Summation of all the errors/loss. Simple summation! \n",
    "- Averaging of all the errors/loss. Average!\n",
    "\n",
    "**Standard Methods** : \n",
    "\n",
    "- **Mean absolute error (MAE)** -> Mean of absolute errors. \n",
    "    - MAE measures the average magnitude of errors in a set of predictions, without considering their direction.\n",
    "    - MAE treats all errors equally, regardless of their size. This means that it is **less sensitive to outliers** compared to MSE.\n",
    "    - **Interpretation** :  In a business context like stock price prediction, a MAE of 5 dollars implies that, on average, the prediction is off by 5 dollars. It's straightforward and easy to understand.\n",
    "\n",
    "- **Mean squared error (MSE)** -> Mean of squared errors. \n",
    "    - MSE measures the average of the squares of the errors. \n",
    "    - It **accentuates (emphasize) larger errors more than smaller ones** since the errors are squared before they are averaged.\n",
    "    - MSE is **highly sensitive to outliers**. A few large errors can significantly increase the value of MSE, making it useful for models where large errors are particularly undesirable.\n",
    "    - The squaring is done to penalise the one having more error much analogous to the subject in which the student scored the least need to be focussed more because that subject has the most error i.e. deviation from the ideal i.e 100. Further square makes the error positive. \n",
    "    - **Interpretation** : An error of 5 dollars yields an MSE of 25 (since (5^2 = 25)). This amplification illustrates that MSE penalizes larger errors more heavily. Thus, MSE can overstate the impact of errors compared to MAE.\n",
    "\n",
    "- **Root mean square error (RMSE)** -> The root of Mean of squared errors. \n",
    "    - RMSE is the square root of MSE\n",
    "    - It brings the error metric back to the same scale as the original data, which is not the case with MSE since it squares the errors.\n",
    "    - Used **when MSE is giving very large values**.\n",
    "    - RMSE is useful for its interpretability. Since it's on the same scale as the target variable, stakeholders can understand it more intuitively than MSE.\n",
    "    - **Interpretation** : if RMSE is 5, it means that the model's predictions are, on average, within 5 of the actual stock prices. It's more interpretable than an MSE of  25 (which is 5 squared), as it directly relates to the scale of the stock prices.\n",
    "\n",
    "- Modified Mean squared error (mMSE) -> kind of half of RMSE. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**When RMSE and when MAE:**\n",
    "- Use MAE when:\n",
    "\n",
    "    - You care about typical performance\n",
    "    - Outliers are not critical\n",
    "    - You want a stable and explainable metric\n",
    "\n",
    "- Use RMSE when:\n",
    "\n",
    "    - Large errors are unacceptable\n",
    "    - Outliers represent true risk\n",
    "    - You want the model to â€œplay safe\n",
    "\n",
    "- In real-world ML pipelines:\n",
    "\n",
    "    - Train using RMSE (or MSE) for stronger gradients\n",
    "    - Report MAE for business stakeholders\n",
    "    - Monitor both to understand error behavior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Types of evaluation metrics (FOR CLASSIFICATION): \n",
    "- True Positive (TP)\n",
    "- True Negative (TN)\n",
    "- False Positive (FP)\n",
    "- False Negative (FN)\n",
    "\n",
    "These all work on each line item of the dataset and using these we will define the evalutaion matrix. \n",
    "- Accuracy : (TP+TN)/all i.e. all true cases / all cases. Overall correctness\n",
    "- Precision : TP/(TP+FP) i.e. How often we are correct.\n",
    "- Recall : TP/(TP+FN) i.e. How many actual spams got caught\n",
    "- F1 score : Harmonic mean : a balance of precision and recall.\n",
    "\n",
    "  F1 = 2*precision *recall/(precision + recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a9bd63",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "420624ba",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0b12e426",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fb311231",
   "metadata": {},
   "source": [
    "**Performance metrics**: These tell you if your model is working properly or not. \n",
    "- Confusion matrix \n",
    "- Accuracy\n",
    "- Precision\n",
    "- Recall\n",
    "- F-beta score\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62411ffe",
   "metadata": {},
   "source": [
    "**Confusion matrix** for a logistic regression problem is a 2x2 matrix. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ec3e2a",
   "metadata": {},
   "source": [
    "- **Accuracy** = (TP+TN)/(All)\n",
    "For imbalanced datasets you usually use precision and recall. If you do your model will say an accuracy of 90% but in actual it won't be 90%. \n",
    "\n",
    "- **Precision** : (TP)/(TP+FP). Out of all the actual values, how many were correctly predicted? \n",
    "\n",
    "    - We try to reduce the false positives here, like we don't want a non psam email to be modelled aas spam i.e we don't false positives. \n",
    "\n",
    "\n",
    "- **Recall** :  (TP)/(TP+FN) . Out of all the predicted values, how many were correctly predicted? \n",
    "    - We try to reduce false negatives here liuke we don't want a person actually having diabetes but the model categorising as not having diabetes. \n",
    "\n",
    "- **F-Beta score** - If FP & FN are both important, we take beta =1 and then we have a F1 score, \n",
    "    - f1 sCORE = 2 X PRECISION X RECALL/(PRECISION + RECALL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d0f185",
   "metadata": {},
   "source": [
    "cddcdc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f3e3e22",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
