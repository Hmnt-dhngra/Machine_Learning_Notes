{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59595cc4-178f-4111-8c6a-6c0cf2d32fb5",
   "metadata": {},
   "source": [
    "#### **Introduction to Machine Learning**\n",
    "\n",
    "Imagine teaching a child to recognize animals. You show them pictures of cats and dogs, telling them which is which. After seeing many examples, the child learns to identify new animals they've never seen before. Machine Learning works the same way!\n",
    "\n",
    "\n",
    "Machine Learning (ML) is a **subfield of artificial intelligence** that focuses on building systems that **learn patterns from data** and improve their performance over time **without being explicitly programmed for every scenario**. \n",
    "Instead of writing fixed rules, we provide data and allow algorithms to infer the underlying relationships.\n",
    "\n",
    "\n",
    "**Why Was Machine Learning Needed?**\n",
    "\n",
    "Machine learning emerged because traditional rule-based programming reached its limits in real-world problems. \n",
    "- Rule-Based Systems Do Not Scale\n",
    "    - Developers manually define rules: if condition → then action\n",
    "    - This works only when rules are simple, stable, and well-defined\n",
    "- Data became abundant\n",
    "    - With the rise of internet Sensors and IoT devices, Mobile applications, Enterprise systems we started generating massive volumes of data. If data is available, it is often easier and more effective to learn patterns from data than to hard-code expert knowledge. Machine learning leverages this data to automatically extract insights.\n",
    "- Many Problems Lack Explicit Algorithms\n",
    "    - Fraud detection, recommendation systems etc do not really have a clear logical solutionm and hence we cannot write determinitic algos , ML provides a way to approximate complex functions from input to output. \n",
    "- Improved Performance Over Time\n",
    "    - Unlike traditional programs ML models can improve as more data becomes available and Performance increases without rewriting code\n",
    "    - Example: Search engines, recommender systems, and voice assistants become more accurate as usage grows.\n",
    "\n",
    "**Bottom Line :**  You should be able to take decisions backed by data. You cannot hard code all the complexities in terms of the coded logics . "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e85aae",
   "metadata": {},
   "source": [
    "#### **Evolution of Machine learning**\n",
    "- Rule-Based Systems (1950s–1970s)\n",
    "    - Early AI relied on hand-crafted rules written by humans. These systems worked only for simple, well-defined problems and failed to scale to real-world complexity.\n",
    "\n",
    "- Statistical & Probabilistic Methods (1980s–1990s)\n",
    "    - Focus shifted to statistics and probability. Algorithms such as linear regression, decision trees, k-nearest neighbors, and Naive Bayes learned patterns from data rather than fixed rules.\n",
    "\n",
    "- Classical Machine Learning (1990s–2010)\n",
    "    - More robust algorithms emerged, including SVMs, ensemble methods (bagging, boosting, random forests), and improved optimization techniques. ML became practical due to better computing power and data availability.\n",
    "\n",
    "- Big Data & Feature Engineering Era (2010–2015)\n",
    "    - Explosion of data and distributed computing (Hadoop, Spark). Performance depended heavily on manual feature engineering combined with classical ML models.\n",
    "\n",
    "- Deep Learning Era (2012–Present)\n",
    "    - Neural networks with many layers (CNNs, RNNs, Transformers) enabled automatic feature learning. Major breakthroughs occurred in vision, speech, and natural language processing.\n",
    "\n",
    "- Modern ML Systems (Present)\n",
    "    - Focus expanded beyond models to end-to-end systems: MLOps, scalable training, real-time inference, foundation models, and responsible AI.\n",
    "\n",
    "In one line:\n",
    "Machine learning evolved from hand-coded rules → statistical learning → classical ML → deep learning → large-scale intelligent systems.\n",
    "\n",
    "\n",
    "**Ques:** If machine learning was there from 1950s why do we see its increased usage now?\n",
    "\n",
    "**Ans:** \n",
    "- Amount of data increased significantly \n",
    "- Computation power increased drastcially recently \n",
    "- Computation costs decresed tremendously decresed very recently"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9587a602-79dc-484e-8863-a8485b5ce03c",
   "metadata": {},
   "source": [
    "Data --> EDA --> FE--> ML--> find features --> Task complete\n",
    "\n",
    "If you have new set of features coming up, just do the FE again give that to ML model and it will come up with solutions...You donot need to write all the 100 lines of code again. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "208ad224",
   "metadata": {},
   "source": [
    "#### **Machine Learning is a Subset of Artificial Intelligence**\n",
    "\n",
    "**Artificial Intelligence** is concerned with building systems that can perform tasks that typically require human intelligence like reasoning, planning and ultimately decision making. \n",
    "\n",
    "**Machine Learning** is a subset of AI that focuses specifically on *enabling systems to learn from data and improve performance over time* without explicit rule-based programming. Generally speaking Machine learning is what is used to achieve Artificial intelligence. \n",
    "\n",
    "**Show venn diagram for AI, ML, DL, etc.** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb697496-2ac8-464e-b885-714c4311bacf",
   "metadata": {},
   "source": [
    "#### **Types of Machine Learning**\n",
    "\n",
    "**What is Learning?**\n",
    "\n",
    "Learning simply means finding patterns and when these are done automatically (by the machine) we call this process machine learning. \n",
    "\n",
    "There are 3 methods in which the machine is learning \n",
    "\n",
    "- **Supervised learning** : As the name suggests -- supervised!! means you gonna have some teacher. (labelled data i.e. input - output pairs)\n",
    "  - When members of a team need feedback of the supervisor to confirm if they are doing the work correctly. \n",
    "  - Similarly understand the Features as questions and the target as the answers , so we are telling the machine that so is the answer when this question comes up.\n",
    "  - Features are like the input variables or known as independant variables. \n",
    "  - Targets are the labels or the known output or the ground truth also known as the dependant variable. \n",
    "  - Example : For deciding the income in a new job, you will need to know the applicant's age, years of experience and tech stack, hence all these are indepdant and these together defines the income which is why income is dependant\n",
    "  - Real life examples: House price prediction, Eligible for credit card\n",
    "  - Question we ask from ML model: Given data, **predict some answer**\n",
    "  \n",
    "\n",
    "- **Unsupervised learning** : Learning without labels. \n",
    "  - The model finds hidden patterns on its own, and then makes groups like grouping similar customers together. \n",
    "  - Features --> ML --> groups.\n",
    "  - Question we ask from ML model: Given the data, **segment into the groups.**   \n",
    "\n",
    "\n",
    "- **Reinforcement learning** : Closest to how the humans learn. \n",
    "  - An agent learns by interacting with an environment and receiving rewards or penalties.\n",
    "  - Example : Autonomous cars, robotics, playing chess, etc. \n",
    "  - Won't be used ever in our jobs. \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e99c3a31-cf92-49a8-bf43-7f426fa8b354",
   "metadata": {},
   "source": [
    "### **Machine learning prep**\n",
    "Analogy : \n",
    "- A teacher wants to evaluate whether a student is smart or not. \n",
    "- She teaches 10 questions to the students. \n",
    "- Now there are 2 methods to identify : Give same 10 questions in the test or give new questions in the test \n",
    "    - If the students is asked those same 10 questions, they moght have memorised them and give a false image of being smart i.e. they can perform 100%. \n",
    "    - For hollistic evaluation of a student it is important to have completely new questions because it checks thorough understanding.\n",
    "- Similarly when we provide data to the model and asks it to learn the patterns, it can memorise all the existent patterns and perform 100% perfect, but when that will be given new data , it **might** underperform. \n",
    "- In order to avoid this, we split the data into two parts, one of which the model will see and learn from called Train dataset and the other it doesn't see which is called test dataset. \n",
    "- Now when the model is tested on the unseen data, it will show the actual intellect or smartness of the model in real world. \n",
    "\n",
    "This process of dividiing the data into 2 (or 3) parts is called **Data Splitting**\n",
    "\n",
    "Now you can also do it slightly better by splitting in 3 parts say for 100 records\n",
    "- Train set (70) : Study material\n",
    "- Validation set (20) : Weekly test\n",
    "- Test set (10) : Final exam\n",
    "\n",
    "This is like you teach a student, validate their learnings in unit tests and improve and ultimately test in the annual exams. \n",
    "\n",
    "General rule of thumb for train test split . 70-80 % Train, 20-30% test when you have sufficiently high data rows, else you may even do 95-5 for cases where you have less data.  \n",
    "General rule of thumb for train validate test split . 70% Train, 20% validate , 10 % test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee0c469-9f6d-4ad7-9f76-9d9d37d138e8",
   "metadata": {},
   "source": [
    "#### 1. Data splitting "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ac3534-f94d-4916-8904-82e45a11e060",
   "metadata": {},
   "source": [
    "General Steps:\n",
    "- Make a model using train dataset. \n",
    "- Check the model accuracy on the validation data and observe what you get say 70 %. \n",
    "- If this is not acceptable, change some parameters, again check model accuracy on validation dataset and observe the model accuracy say it to be 76%. ACCEPTABLE!\n",
    "- Now we will test on the test data and check the accuracy --> say 72% i.e. very near to the validation set \n",
    "- PERFECT!!\n",
    "\n",
    "If it is away from the validation set accuracy, we will need to do some tweaks. \n",
    "- Start by checking the constituents of each of the three dataset i.e. test, validate and train \n",
    "- There is a possibility of data imbalance or skewness. \n",
    "- Do a mix of the different types of data points observed by EDA done above and then ultimately train the model with the mixed data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7261c3e-1ea2-4e74-adca-7a9990917946",
   "metadata": {},
   "source": [
    "#### **Errors, Cost Function and Loss Function**\n",
    "for x1=2, x2=3 you have y = 10 and ML predicted y(hat)=3\n",
    "\n",
    "There needs to be a mechanism that tells how wrong the predicted value is versus the actual value. \n",
    "This functionality of comparing is called **evaluation** and you make **evaluation metrics** which help you **decide how well is the model learning** or is it a good model or not. \n",
    "\n",
    "- **Error** is the difference between the actual value and the predicted value. \n",
    "In the context of Machine learning when we talk about Error calculation, we might be talking in two segments\n",
    "1) Error for each single data point\n",
    "2) Error representing the overall model.  \n",
    "\n",
    "Errors\n",
    "- For each data point ==> We Use Loss function. A **Loss function** (also called as error function) measures how wrong a model's prediction is for a single data point. \n",
    "- for the set of data (train, validation, etc) ==> We use Cost function i.e. We use cost function to calculate total error of your data set. **Cost function** aggregates the loss over all training examples and represents the overall performance of the model. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af1db62-5eae-44cf-9092-e59f9b95c9b6",
   "metadata": {},
   "source": [
    "Say for example you have a train data and you want to check the model performance, you will use cost function which ofcourse internally will use a loss function to calculate loss on each data point. \n",
    "Train set error calculation --> you need a cost function --> you calculate loss on each data point using a loss function (a formula)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ce76f4",
   "metadata": {},
   "source": [
    "#### **Underfitting versus Overfitting**\n",
    "\n",
    "Underfitting and Overfitting are the modelling errors related to how well a model learns patterns from data and generalises the unseen data.\n",
    "\n",
    "- If training loss << test/validation loss ====> Model **Over fitting**. \n",
    "    - The model has learnt all the patterns in the training set but couldn't capture the intricacies of the test data. We perform superb on the training data but poorly on the test data. \n",
    "\n",
    "- If training loss == Test/validation loss, that's an ideal case. \n",
    "    - Though it's not a technical term, it is used for a general clarity.\n",
    "\n",
    "- If training loss >>> test /validation loss ===> Model **Underfitting**. \n",
    "    - Usually here both training loss and test/validation loss is bad. \n",
    "The model is not understanding the basic patterns in the training data.\n",
    "\n",
    "\n",
    "**Underfitting** : This occurs when a model is too simple to capture the underlying patterns or structures in data. \n",
    "- Key characteristics \n",
    "    - Poor performance on training data\n",
    "    - Poor performance on test/validation data\n",
    "    - High bias, low variance\n",
    "    - Model fails to learn important relationships\n",
    "\n",
    "- Common Causes\n",
    "\n",
    "    - Model complexity is too low (e.g., linear model for highly non-linear data)\n",
    "    - Insufficient or irrelevant features\n",
    "    - Excessive regularization\n",
    "    - Inadequate training time\n",
    "\n",
    "- Example\n",
    "    - Fitting a straight line (linear regression) to data that clearly follows a curved relationship.\n",
    "\n",
    "- Symptoms\n",
    "    - Training error: High\n",
    "    - Test error: High  \n",
    "\n",
    "**Overfitting** : Overfitting occurs when a model learns the training data too well, including noise and random fluctuations, and fails to generalize to new data.\n",
    "\n",
    "- Key Characteristics\n",
    "    - Excellent performance on training data\n",
    "    - Poor performance on test/validation data\n",
    "    - Low bias, high variance\n",
    "    - Model memorizes instead of generalizing\n",
    "\n",
    "- Common Causes\n",
    "\n",
    "    - Model complexity is too high\n",
    "    - Too many parameters relative to the dataset size\n",
    "    - Training for too many epochs\n",
    "    - Lack of regularization\n",
    "\n",
    "- Example\n",
    "    - A very deep decision tree that perfectly classifies training data but performs poorly on unseen data.\n",
    "\n",
    "- Symptoms\n",
    "    - Training error: Very low\n",
    "    - Test error: High"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "982b6252-3639-44f2-85b2-2765b68b4741",
   "metadata": {},
   "source": [
    "In Train, the model has 30 % accuracy, in test the accuracy is 50 % . Why ?\n",
    "2 factors possible \n",
    "- sample is small\n",
    "- Randomness. Those cases of train where the model is performing good are part of test also 'majorly', the test accuracy will be better. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f107f35e-4f8d-44bc-b631-101e114f0254",
   "metadata": {},
   "source": [
    "**Types of evaluation metrics (FOR REGRESSION):** \n",
    "- summation of all the errors/loss. Simple summation! \n",
    "- averaging of all the errors/loss. Average!\n",
    "- Mean squared error (MSE) -> Mean of squared errors. The squaring is done to penalise the one having more error much analogous to the subject in which the student scored the least need to be focussed more because that subject has the most error i.e. deviation from the ideal i.e 100.\n",
    "- Mean absolute error (MAE) -> Mean of absolute errors.\n",
    "- Root mean square error (RMSE) -> The root of Mean of squared errors. Used when MSE is giving very large values.\n",
    "- Modified Mean squared error (mMSE) -> kind of half of RMSE. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e73768-5ede-4d89-999c-cebb1f946c56",
   "metadata": {},
   "source": [
    "Types of evaluation metrics (FOR CLASSIFICATION): \n",
    "- True Positive (TP)\n",
    "- True Negative (TN)\n",
    "- False Positive (FP)\n",
    "- False Negative (FN)\n",
    "\n",
    "These all work on each line item of the dataset and using these we will define the evalutaion matrix. \n",
    "- Accuracy : (TP+TN)/all i.e. all true cases / all cases. Overall correctness\n",
    "- Precision : TP/(TP+FP) i.e. How often we are correct.\n",
    "- Recall : TP/(TP+FN) i.e. How many actual spams got caught\n",
    "- F1 score : Harmonic mean : a balance of precision and recall.\n",
    "\n",
    "  F1 = 2*precision *recall/(precision + recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb45987d-0024-47fe-b1a2-4110a164b427",
   "metadata": {},
   "source": [
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c03f204a-a81a-41fd-b281-05859873f824",
   "metadata": {},
   "source": [
    "Linear Regression is called linear not because of straight line but how you treat the parameters as in in a linear regression you can represnt the parameters like a simple summation or subtraction. \n",
    "A polynomial looking graph can also be a linear regression because it can have linear features inside that. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "421c3819-a962-4610-af82-0481616f0a44",
   "metadata": {},
   "source": [
    "### Flow of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2614e4d-2144-405b-b807-022b4417eea6",
   "metadata": {},
   "source": [
    "Data --> EDA --> FE --> final data\n",
    "\n",
    "Final data --> Train and test \n",
    "\n",
    "Train --> formula say y = mx + c . We start with a random initial value of m & c --> prediction --> error --> gradient "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00aa5f6d-c9b3-4882-9d3e-d874fe7d8a22",
   "metadata": {},
   "source": [
    "**Cost function** is the curve obtained when you plot diffeerent values of losses for different values of m1, m2, etc which machine learning finds.\n",
    "\n",
    "https://aero-learn.imperial.ac.uk/vis/Machine%20Learning/gradient_descent_3d.html\n",
    "\n",
    "Ultimately we want to reduce the loss as much as possible possibly zero, We do this by the method of gradient descent. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d0558f-b5b1-4b79-97cc-cb5c9fc63405",
   "metadata": {},
   "source": [
    "For each value of parameters we get different values of Loss. \n",
    "\n",
    "If we plot all the loss values against the parameter values you get a 3D function as shown in above URL. \n",
    "\n",
    "Ideally we want to have the lowest possible value of Lowest loss, gradient descent is the method that gives you the values of m0, m1, m2 WHICH GIVES THE LOWEST VALUE OF LOSS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ffd023-9b34-4b11-bb6d-42981bd1aedd",
   "metadata": {},
   "source": [
    "**Gradient Descent** : It is an optimisation algorithm used in Machine learning. It is an iterative method used to adjust the parameters if a model to find the minimum loss of a function "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b8e379-c0f0-4bfc-9fd0-d4048107af26",
   "metadata": {},
   "source": [
    "Loss + loss function ===> Gradient descent = gradient ====> update values of m0, m1, m2. Find the new loss, give to gradient descent and so on.... keep optimising such that the loss no longer decreases. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2364b485-5a47-446a-ae65-519301e014e2",
   "metadata": {},
   "source": [
    "https://www.playwithml.com/concepts/gradient-descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49896f9d-2a35-44b7-8d5a-c0e98270733d",
   "metadata": {},
   "source": [
    "Goal of Gradient Descent is to find minimum loss. \n",
    "- We use differentiation to calculate slope (gradient) of the cost function at our current position\n",
    "- Slope/gradient will tell us the direction of the steepest ascent (uphill)\n",
    "- Take a step in the opposite direction i.e. towards steepest descent (downhill)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b1ffea8-20bf-4ad9-9dd4-509d03b3888d",
   "metadata": {},
   "source": [
    "Differentiation is simply what will be the rate of change of output when the input changes i.e. value of slope at that point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ebe12ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58537daf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
