{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0770db6",
   "metadata": {},
   "source": [
    "#### **Ensemble Methods**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee4b146",
   "metadata": {},
   "source": [
    "Ensemble techniques are machine learning methods that **combine predictions from multiple models (base learners)** to produce a single, more robust and accurate prediction than any individual model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e41dd84",
   "metadata": {},
   "source": [
    "#### **Why Ensemble Methods Work**\n",
    "**Core Idea** : A group of diverse, reasonably accurate models can outperform a single strong model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb39b09e",
   "metadata": {},
   "source": [
    "Single models often suffer from one or more of the following issues:\n",
    "\n",
    "**1) High variance**\n",
    "- Model is very sensitive to training data (e.g., decision trees).\n",
    "- Small changes in data lead to large changes in predictions.\n",
    "\n",
    "**2) High bias**\n",
    "- Model is too simplistic and underfits the data (e.g., linear models for complex problems).\n",
    "\n",
    "**3) Instability**\n",
    "- Some algorithms produce inconsistent results depending on sampling.\n",
    "\n",
    "**4) Limited generalization**\n",
    "- A single hypothesis may not capture all patterns in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca064b5",
   "metadata": {},
   "source": [
    "**Key insight**: Each model makes DIFFERENT errors!\n",
    "\n",
    "- Model A: Correct on samples [1,2,3,5,7,9] - 60% accuracy\n",
    "\n",
    "- Model B: Correct on samples [2,4,5,6,8,10] - 60% accuracy\n",
    "\n",
    "- Model C: Correct on samples [1,3,4,6,7,8] - 60% accuracy\n",
    "\n",
    "Majority vote: Correct on [1,2,3,4,5,6,7,8] - 80% accuracy!\n",
    "\n",
    "**Diversity reduces variance!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e5f6d6",
   "metadata": {},
   "source": [
    "**Key Intuition (Bias - Variance Trade-Off)**\n",
    "\n",
    "Ensemble methods improve performance by:\n",
    "\n",
    "- Reducing variance (e.g., Bagging)\n",
    "\n",
    "- Reducing bias (e.g., Boosting)\n",
    "\n",
    "- Improving overall generalization by averaging or combining errors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e738e410",
   "metadata": {},
   "source": [
    "#### **Main Types of Ensemble methods:**\n",
    "- Bagging\n",
    "\n",
    "- Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e7ba8fd",
   "metadata": {},
   "source": [
    "**Bagging (Bootstrap Aggregating)** : Mainly reduces variance\n",
    "\n",
    "**How it works:**\n",
    "We create multiple models and aggregate (Either take majority or take mean/median, etc.) their predictions. \n",
    "\n",
    "- Create multiple training datasets using bootstrap sampling (sampling with replacement).\n",
    "\n",
    "- Train the same algorithm on each dataset.\n",
    "\n",
    "- Aggregate predictions (majority vote for classification, mean for regression).\n",
    "\n",
    "**Best suited for:**\n",
    "- High variance models like decision trees\n",
    "\n",
    "**Example:** \n",
    "- Random Forest\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "103e6270",
   "metadata": {},
   "source": [
    "**Boosting ()**\n",
    "\n",
    "**How it works:**\n",
    "We create a model in sequence. The performance of previous model is inherited in the sequence and developments on top of it is done so that the final model performs the best. \n",
    "\n",
    "- Models are trained sequentially.\n",
    "\n",
    "- Each new model focuses more on previously misclassified samples.\n",
    "\n",
    "- Final prediction is a weighted combination of all models.\n",
    "\n",
    "**Best suited for:** \n",
    "- Weak learners that perform slightly better than random guessing\n",
    "\n",
    "**Example:**\n",
    "- Adaboost\n",
    "- Gradient Boosting\n",
    "- XGBoost : Usually the best models or atleast in top 3 models\n",
    "- LightGBM\n",
    "- CatBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7886bf2",
   "metadata": {},
   "source": [
    "#### **Bootstrap Dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df2b21e3",
   "metadata": {},
   "source": [
    "A bootstrap dataset is a same-sized dataset created by sampling the original data with replacement, allowing duplicates and omissions, and is used to improve model stability and generalization.\n",
    "\n",
    "It is a foundational concept behind bagging-based ensemble methods such as Random Forest.\n",
    "\n",
    "A bootstrap dataset is constructed by:\n",
    "- Randomly selecting ùëõ samples with replacement from original dataset ùê∑\n",
    "\n",
    "Because sampling is with replacement:\n",
    "\n",
    "- Some observations appear multiple times\n",
    "- Some observations are not selected at all"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e9c895",
   "metadata": {},
   "source": [
    "Row\tSquare Feet\t    Price (Lakhs)\n",
    "\n",
    "- A: 900:70\n",
    "- B: 1000:80\n",
    "- C: 900:70\n",
    "- D: 1500:90\n",
    "- E: 1600:95\n",
    "- F: 1700:100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c2ed15f",
   "metadata": {},
   "source": [
    "STEP 1: Create Bootstrap Samples (WITH Replacement)\n",
    "\n",
    "                            Bootstrap Sample 1:\n",
    "                            Randomly pick 6 samples WITH replacement:\n",
    "                            ‚îú‚îÄ Row A: 900 sq ft, ‚Çπ70L (sampled TWICE!)\n",
    "                            ‚îú‚îÄ Row A: 900 sq ft, ‚Çπ70L (duplicate)\n",
    "                            ‚îú‚îÄ Row B: 1000 sq ft, ‚Çπ80L\n",
    "                            ‚îú‚îÄ Row D: 1500 sq ft, ‚Çπ90L\n",
    "                            ‚îú‚îÄ Row E: 1600 sq ft, ‚Çπ95L\n",
    "                            ‚îî‚îÄ Row F: 1700 sq ft, ‚Çπ100L\n",
    "\n",
    "                            Bootstrap Sample 2:\n",
    "                            ‚îú‚îÄ Row C: 900 sq ft, ‚Çπ70L\n",
    "                            ‚îú‚îÄ Row D: 1500 sq ft, ‚Çπ90L\n",
    "                            ‚îú‚îÄ Row E: 1600 sq ft, ‚Çπ95L\n",
    "                            ‚îú‚îÄ Row E: 1600 sq ft, ‚Çπ95L (sampled TWICE!)\n",
    "                            ‚îú‚îÄ Row F: 1700 sq ft, ‚Çπ100L\n",
    "                            ‚îî‚îÄ Row B: 1000 sq ft, ‚Çπ80L\n",
    "\n",
    "                            Bootstrap Sample 3:\n",
    "                            ‚îú‚îÄ Row F: 1700 sq ft, ‚Çπ100L\n",
    "                            ‚îú‚îÄ Row C: 900 sq ft, ‚Çπ70L\n",
    "                            ‚îú‚îÄ Row E: 1600 sq ft, ‚Çπ95L\n",
    "                            ‚îú‚îÄ Row A: 900 sq ft, ‚Çπ70L\n",
    "                            ‚îú‚îÄ Row B: 1000 sq ft, ‚Çπ80L\n",
    "                            ‚îî‚îÄ Row D: 1500 sq ft, ‚Çπ90L\n",
    "                        \n",
    "\n",
    "                        \n",
    "**This is like different decision trees on different subset of data, each with their own prediction**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7820a0a7",
   "metadata": {},
   "source": [
    "STEP 2: Train Separate Model on Each Sample\n",
    "\n",
    "                            Tree 1: Trained on Sample 1\n",
    "                            ‚Ä¢ Learns splits based on its data\n",
    "                            ‚Ä¢ For 950 sq ft ‚Üí Predicts: ‚Çπ75L\n",
    "\n",
    "                            Tree 2: Trained on Sample 2\n",
    "                            ‚Ä¢ Different data ‚Üí Different splits!\n",
    "                            ‚Ä¢ For 950 sq ft ‚Üí Predicts: ‚Çπ72L\n",
    "\n",
    "                            Tree 3: Trained on Sample 3\n",
    "                            ‚Ä¢ Yet another perspective\n",
    "                            ‚Ä¢ For 950 sq ft ‚Üí Predicts: ‚Çπ78L"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d5c3a6",
   "metadata": {},
   "source": [
    "STEP 3: Aggregate Predictions (Average)\n",
    "\n",
    "                            For test property with 950 sq ft:\n",
    "\n",
    "                            Prediction‚ÇÅ = ‚Çπ75L\n",
    "                            Prediction‚ÇÇ = ‚Çπ72L\n",
    "                            Prediction‚ÇÉ = ‚Çπ78L\n",
    "\n",
    "                            Final Bagging Prediction:\n",
    "                            Average = (75 + 72 + 78) / 3\n",
    "                            = 225 / 3\n",
    "                            = ‚Çπ75 Lakhs ‚úì\n",
    "\n",
    "                            Why it works:\n",
    "                            ‚Ä¢ Each tree makes slightly different errors\n",
    "                            ‚Ä¢ Averaging reduces overall variance\n",
    "                            ‚Ä¢ More stable than single tree!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b812f1e",
   "metadata": {},
   "source": [
    "**Out-of-Bag (OOB)** data refers to the **subset of original training samples that are not selected in a particular bootstrap dataset** during ensemble training.\n",
    "\n",
    "It is a direct consequence of **bootstrap sampling with replacement** and is primarily used in bagging-based models, especially Random Forest."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4850a7b9",
   "metadata": {},
   "source": [
    "**Random forest is a self validating model because each bootstrapped data is used for training and the OOB data for that bootstrapped dataset is tested so the training and testing is being done simultaneously**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "972ec2b3",
   "metadata": {},
   "source": [
    "#### **Random Forest**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e634cd16",
   "metadata": {},
   "source": [
    "Random Forest is a **supervised ensemble learning algorithm** used for **Classification as well as Regression** that builds **multiple decision trees** and combines their predictions to produce a more accurate, stable, and generalizable model.\n",
    "\n",
    "It is based on:\n",
    "\n",
    "- Bagging (Bootstrap Aggregation)\n",
    "- Random feature selection\n",
    "\n",
    "Intuitive definition of RANDOM FOREST\n",
    "- Random = RANDOM FEATURE SELECTION + RANDOM DATASET SAMPLING\n",
    "- Forest = Multiple Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "658ee3b2",
   "metadata": {},
   "source": [
    "**Why Random Forest Was Needed**\n",
    "\n",
    "**Problem with Decision Trees**\n",
    "\n",
    "Decision Trees:\n",
    "- Have low bias\n",
    "- Have very high variance\n",
    "- Tend to overfit the training data\n",
    "\n",
    "**Bagging Alone Is Not Enough**\n",
    "\n",
    "Bagging reduces variance by training trees on different bootstrap datasets, but:\n",
    "\n",
    "- Trees can still be highly correlated\n",
    "- Correlated errors reduce ensemble effectiveness\n",
    "\n",
    "**Random Forest Solution**\n",
    "\n",
    "Random Forest:\n",
    "\n",
    "- Uses bootstrap sampling (data randomness)\n",
    "- Uses feature subsampling (model randomness)\n",
    "\n",
    "This decorrelates trees, significantly improving performance.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fddc0671",
   "metadata": {},
   "source": [
    "**Core Ideas Behind Random Forest**\n",
    "\n",
    "Random Forest reduces variance by:\n",
    "\n",
    "- Training many decision trees on different bootstrap datasets\n",
    "- Restricting each tree to consider only a random subset of features at each split\n",
    "- Aggregating predictions across all trees"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a87f1e8c",
   "metadata": {},
   "source": [
    "**Steps**\n",
    "\n",
    "1) Bootstrap Sampling : For each tree: Sample  ùëõ rows with replacement. Remaining rows become Out-of-Bag (OOB) samples. For each tree you will have a bootstrap data and OOB data which are basically remainng rows apart from bootstrap data\n",
    "\n",
    "2) Random Feature selection & CReating trees : \n",
    "- Select at random a few features less than total features. \n",
    "    - Calculate parent variance\n",
    "    - Sort data and find mid points\n",
    "    - For each mid point\n",
    "        - Split from mid point \n",
    "        - Calculate variance of right and left side\n",
    "        - Calculate weighted variance\n",
    "        - Calculate variance reduction \n",
    "    - Find the best mid point using the highest variance reduction. \n",
    "\n",
    "    Each tree will predict some value and ultimately we take average of all those predictions and that becomes the final answer. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c3f9ab5",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
